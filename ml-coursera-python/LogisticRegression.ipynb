{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding and implementing Vectorized Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will be an attempt to implement logistic regression to predict each applicant’s chance of admission based on their results on two exams. The data used is from Andrew Ng's ML course and what follows below is the course assignment completed in Python as opposed to Octave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 3 columns):\n",
      "Test1     100 non-null float64\n",
      "Test2     100 non-null float64\n",
      "Result    100 non-null int64\n",
      "dtypes: float64(2), int64(1)\n",
      "memory usage: 2.4 KB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/ex2data1.txt', header = None, names = ('Test1', 'Test2', 'Result'))\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test1</th>\n",
       "      <th>Test2</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.623660</td>\n",
       "      <td>78.024693</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.286711</td>\n",
       "      <td>43.894998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35.847409</td>\n",
       "      <td>72.902198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60.182599</td>\n",
       "      <td>86.308552</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79.032736</td>\n",
       "      <td>75.344376</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Test1      Test2  Result\n",
       "0  34.623660  78.024693       0\n",
       "1  30.286711  43.894998       0\n",
       "2  35.847409  72.902198       0\n",
       "3  60.182599  86.308552       1\n",
       "4  79.032736  75.344376       1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the hypothesis function takes the form hθ(x)=θ0+θ1x1+θ2x2+θ3x3+⋯+θnxn\n",
    "With vectorization we will assume that input training data x0 will be 1. \n",
    "\n",
    "Also we will init the X, y and theta variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training data - X and result - y\n",
    "data.insert(0, 'Test0', 1)\n",
    "cols = data.shape[1]\n",
    "X = data.iloc[:,:cols-1]\n",
    "y = data.iloc[:,cols-1:cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Test0      Test1      Test2\n",
      "0      1  34.623660  78.024693\n",
      "1      1  30.286711  43.894998\n",
      "2      1  35.847409  72.902198\n",
      "3      1  60.182599  86.308552\n",
      "4      1  79.032736  75.344376\n",
      "   Result\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       1\n",
      "4       1\n",
      "(100, 3)\n",
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X.head())\n",
    "print(y.head())\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the cost/loss function - which is\n",
    "cost(hθ(x), y) = 1/m * (-y.log(hθ(x))-(1-y).log(1-hθ(x)))\n",
    "\n",
    "Before that some transformation\n",
    "\n",
    "X - 118x3 theta - 3x1\n",
    "theta (T) - 1x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X.values)  \n",
    "y = np.array(y.values)  \n",
    "theta = np.zeros(3)\n",
    "\n",
    "print(theta.shape)\n",
    "\n",
    "X = np.matrix(X)\n",
    "y = np.matrix(y)\n",
    "theta = np.matrix(theta)\n",
    "    \n",
    "print(theta.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence the transformation as\n",
    "\n",
    "X - 118x3 theta - 1x3\n",
    "\n",
    "So we need a transpose so that theta(T) - 3x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(theta, X, y):\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    theta = np.matrix(theta)\n",
    "    \n",
    "    #print(\"X in cost\",X.shape)\n",
    "    #print(\"theta in cost\", theta.shape)\n",
    "    prob1 = np.multiply(-y, np.log(sigmoid(X * theta.T)))\n",
    "    prob2 = np.multiply((1-y), np.log(1-sigmoid(X * theta.T)))\n",
    "\n",
    "    return np.sum(prob1-prob2)/(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69314718055994529"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(theta, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats the cost with theta = all zeros\n",
    "\n",
    "Now we need a gradient function to arrive at the best parameters with the least cost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "parameters = int(theta.ravel().shape[1])\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single gradient step - \n",
    "θ:=θ−α/mXT(g(Xθ)−y)\n",
    "\n",
    "[  8.47457627e-03,   1.87880932e-02,   7.77711864e-05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient(theta, X, y):\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    theta = np.matrix(theta)\n",
    "    \n",
    "    #Get the number of parameters\n",
    "    parameters = int(theta.ravel().shape[1])\n",
    "    grad = np.zeros(parameters)\n",
    "    #print (X.shape)\n",
    "    #print (theta.T.shape)\n",
    "    error = sigmoid(X * theta.T) - y\n",
    "\n",
    "    for i in range(parameters):\n",
    "        term = np.multiply(error, X[:,i])\n",
    "        grad[i] = np.sum(term) / len(X)\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_vec(theta, X, y):\n",
    "    \"\"\"\n",
    "    Return gradient of theta \n",
    "    Args:\n",
    "        theta is a 1D numpy array \n",
    "        X is a m x n numpy array w/ the first column being an intercept of 1\n",
    "        y is a m x 1 numpy array, it is the label set for each obs in X\n",
    "        \n",
    "    \"\"\"\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    theta = np.matrix(theta)\n",
    "    m = len(y)\n",
    "    h_theta = sigmoid(X.dot(theta.T))\n",
    "    \n",
    "    grad = (h_theta - y.T).T.dot(X)/m\n",
    "      \n",
    "    \n",
    "    return np.ndarray.flatten(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(X, y, theta, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (n_x, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (n_x, m)\n",
    "    Y -- true \"label\" vector (containing 0 if class 1, 1 if class 2), of shape (1, m)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \"\"\"\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        cost_i = cost(theta, X, y)\n",
    "        grad = gradient(theta, X, y)\n",
    "        \n",
    "        \n",
    "        theta = theta - learning_rate*grad\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost_i)\n",
    "            \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(i)\n",
    "            print (\"Cost (iteration %i) = %f\" %(i, cost_i))\n",
    "            \n",
    "    \n",
    "        \n",
    "    return theta, grad, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Cost (iteration 0) = 0.693147\n",
      "100\n",
      "Cost (iteration 100) = 0.630171\n",
      "200\n",
      "Cost (iteration 200) = 0.629803\n",
      "300\n",
      "Cost (iteration 300) = 0.629691\n",
      "400\n",
      "Cost (iteration 400) = 0.629629\n",
      "500\n",
      "Cost (iteration 500) = 0.629578\n",
      "600\n",
      "Cost (iteration 600) = 0.629529\n",
      "700\n",
      "Cost (iteration 700) = 0.629480\n",
      "800\n",
      "Cost (iteration 800) = 0.629431\n",
      "900\n",
      "Cost (iteration 900) = 0.629383\n",
      "1000\n",
      "Cost (iteration 1000) = 0.629334\n",
      "1100\n",
      "Cost (iteration 1100) = 0.629285\n",
      "1200\n",
      "Cost (iteration 1200) = 0.629237\n",
      "1300\n",
      "Cost (iteration 1300) = 0.629188\n",
      "1400\n",
      "Cost (iteration 1400) = 0.629139\n",
      "1500\n",
      "Cost (iteration 1500) = 0.629091\n",
      "1600\n",
      "Cost (iteration 1600) = 0.629042\n",
      "1700\n",
      "Cost (iteration 1700) = 0.628993\n",
      "1800\n",
      "Cost (iteration 1800) = 0.628945\n",
      "1900\n",
      "Cost (iteration 1900) = 0.628896\n",
      "2000\n",
      "Cost (iteration 2000) = 0.628847\n",
      "2100\n",
      "Cost (iteration 2100) = 0.628799\n",
      "2200\n",
      "Cost (iteration 2200) = 0.628750\n",
      "2300\n",
      "Cost (iteration 2300) = 0.628702\n",
      "2400\n",
      "Cost (iteration 2400) = 0.628653\n",
      "2500\n",
      "Cost (iteration 2500) = 0.628605\n",
      "2600\n",
      "Cost (iteration 2600) = 0.628556\n",
      "2700\n",
      "Cost (iteration 2700) = 0.628507\n",
      "2800\n",
      "Cost (iteration 2800) = 0.628459\n",
      "2900\n",
      "Cost (iteration 2900) = 0.628410\n",
      "3000\n",
      "Cost (iteration 3000) = 0.628362\n",
      "3100\n",
      "Cost (iteration 3100) = 0.628313\n",
      "3200\n",
      "Cost (iteration 3200) = 0.628265\n",
      "3300\n",
      "Cost (iteration 3300) = 0.628216\n",
      "3400\n",
      "Cost (iteration 3400) = 0.628168\n",
      "3500\n",
      "Cost (iteration 3500) = 0.628119\n",
      "3600\n",
      "Cost (iteration 3600) = 0.628071\n",
      "3700\n",
      "Cost (iteration 3700) = 0.628023\n",
      "3800\n",
      "Cost (iteration 3800) = 0.627974\n",
      "3900\n",
      "Cost (iteration 3900) = 0.627926\n",
      "4000\n",
      "Cost (iteration 4000) = 0.627877\n",
      "4100\n",
      "Cost (iteration 4100) = 0.627829\n",
      "4200\n",
      "Cost (iteration 4200) = 0.627780\n",
      "4300\n",
      "Cost (iteration 4300) = 0.627732\n",
      "4400\n",
      "Cost (iteration 4400) = 0.627684\n",
      "4500\n",
      "Cost (iteration 4500) = 0.627635\n",
      "4600\n",
      "Cost (iteration 4600) = 0.627587\n",
      "4700\n",
      "Cost (iteration 4700) = 0.627538\n",
      "4800\n",
      "Cost (iteration 4800) = 0.627490\n",
      "4900\n",
      "Cost (iteration 4900) = 0.627442\n",
      "5000\n",
      "Cost (iteration 5000) = 0.627393\n",
      "5100\n",
      "Cost (iteration 5100) = 0.627345\n",
      "5200\n",
      "Cost (iteration 5200) = 0.627297\n",
      "5300\n",
      "Cost (iteration 5300) = 0.627248\n",
      "5400\n",
      "Cost (iteration 5400) = 0.627200\n",
      "5500\n",
      "Cost (iteration 5500) = 0.627152\n",
      "5600\n",
      "Cost (iteration 5600) = 0.627103\n",
      "5700\n",
      "Cost (iteration 5700) = 0.627055\n",
      "5800\n",
      "Cost (iteration 5800) = 0.627007\n",
      "5900\n",
      "Cost (iteration 5900) = 0.626958\n",
      "[[-0.04171383  0.01071471  0.00077566]]\n",
      "[ 0.0694691  -0.00048215 -0.00053997]\n",
      "[0.69314718055994529, 0.63017143073310311, 0.62980312317066023, 0.62969076228502785, 0.62962931458032545, 0.62957804153015773, 0.62952881140412043, 0.62947999723185588, 0.62943127313391978, 0.62938257380355667, 0.62933388614938546, 0.62928520754596873, 0.62923653746587338, 0.62918787580225077, 0.62913922253256693, 0.62909057765119103, 0.62904194115588052, 0.62899331304507244, 0.62894469331733982, 0.62889608197128299, 0.62884747900550775, 0.62879888441862097, 0.62875029820922979, 0.62870172037594196, 0.62865315091736451, 0.62860458983210488, 0.62855603711877039, 0.62850749277596885, 0.62845895680230734, 0.62841042919639367, 0.6283619099568355, 0.62831339908224004, 0.62826489657121543, 0.62821640242236898, 0.62816791663430871, 0.62811943920564228, 0.62807097013497737, 0.62802250942092208, 0.62797405706208409, 0.62792561305707151, 0.62787717740449223, 0.62782875010295403, 0.62778033115106535, 0.62773192054743399, 0.62768351829066815, 0.62763512437937619, 0.62758673881216598, 0.62753836158764587, 0.62748999270442429, 0.62744163216110937, 0.62739327995630989, 0.62734493608863362, 0.62729660055668957, 0.62724827335908584, 0.62719995449443122, 0.62715164396133405, 0.6271033417584031, 0.62705504788424715, 0.62700676233747454, 0.62695848511669428]\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 6000\n",
    "learning_rate = 0.0001\n",
    "print_cost = True\n",
    "parameters, grads, costs = optimize(X, y, theta, num_iterations, learning_rate, print_cost)\n",
    "print(parameters)\n",
    "print(grads)\n",
    "print(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cost at theta found by fmin_tnc: 0.203497701589\n",
      "\n",
      "Optimal theta: [-25.16131863   0.20623159   0.20147149]\n"
     ]
    }
   ],
   "source": [
    "import scipy.optimize as opt\n",
    "X = np.matrix(X)\n",
    "y = np.matrix(y)\n",
    "theta = np.zeros(3)  \n",
    "\n",
    "result = opt.fmin_tnc(func=cost, x0=theta, fprime=gradient, args=(X, y))  \n",
    "print ('\\nCost at theta found by fmin_tnc:', cost(result[0], X, y))\n",
    "print ('\\nOptimal theta:',  result[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.203498\n",
      "         Iterations: 23\n",
      "         Function evaluations: 31\n",
      "         Gradient evaluations: 31\n",
      "\n",
      "Cost at theta found by fmin_bfgs: 0.20349770158944389\n",
      "\n",
      "optimal theta:\n",
      "[-25.16133284   0.2062317    0.2014716 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/smritimohapatra/anaconda3/envs/root_clone/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: divide by zero encountered in log\n",
      "  if __name__ == '__main__':\n",
      "/Users/smritimohapatra/anaconda3/envs/root_clone/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in multiply\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "import scipy.optimize as opt\n",
    "initial_theta = np.zeros(3)\n",
    "\n",
    "myargs = (X, y)\n",
    "opts = {'full_output': True, 'maxiter': 400}\n",
    "\n",
    "optimal_theta, cost, grad_at_min, inv_hessian_matrix,\\\n",
    "fun_calls, grad_calls, warn_flags = opt.fmin_bfgs(cost,\n",
    "                                initial_theta,\n",
    "                                args=myargs,\n",
    "                                fprime=gradient,\n",
    "                                **opts)\n",
    "\n",
    "print ('\\nCost at theta found by fmin_bfgs:', cost)\n",
    "print ('\\noptimal theta:')\n",
    "print (optimal_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(theta, X):\n",
    "    result = sigmoid (X * theta.T)\n",
    "    return [1 if x >= 0.5 else 0 for x in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta_min = np.matrix(result[0])  \n",
    "predictions = predict(theta_min, X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an accuracy function to test accuracy of predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a regularized version to help with non-linear relationships. We will start with data which we already know to not have a simple linear relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 118 entries, 0 to 117\n",
      "Data columns (total 3 columns):\n",
      "Test1     118 non-null float64\n",
      "Test2     118 non-null float64\n",
      "Result    118 non-null int64\n",
      "dtypes: float64(2), int64(1)\n",
      "memory usage: 2.8 KB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/ex2data2.txt', header = None, names = ('Test1', 'Test2', 'Result'))\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test1</th>\n",
       "      <th>Test2</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.051267</td>\n",
       "      <td>0.69956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.092742</td>\n",
       "      <td>0.68494</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.213710</td>\n",
       "      <td>0.69225</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.50219</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.513250</td>\n",
       "      <td>0.46564</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Test1    Test2  Result\n",
       "0  0.051267  0.69956       1\n",
       "1 -0.092742  0.68494       1\n",
       "2 -0.213710  0.69225       1\n",
       "3 -0.375000  0.50219       1\n",
       "4 -0.513250  0.46564       1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_features(data, deg=5):\n",
    "    #Going upto 'deg' degrees\n",
    "    x1 = data['Test1']  \n",
    "    x2 = data['Test2']\n",
    "    \n",
    "    data.insert(3, 'Ones', 1)\n",
    "    \n",
    "    for i in range(1, deg):  \n",
    "        for j in range(0, i):\n",
    "            data['In' + str(i) + str(j)] = np.power(x1, i-j) * np.power(x2, j)\n",
    "            \n",
    "    ##Going to need to clean up the other inputs\n",
    "    \n",
    "    ##And as usual lets add 'Ones' to make up for x0 parameter\n",
    "    \n",
    "    data.drop('Test1', axis=1, inplace=True)\n",
    "    data.drop('Test2', axis=1, inplace=True)\n",
    "        \n",
    "    print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Result  Ones      In10      In20      In21      In30      In31      In32  \\\n",
      "0       1     1  0.051267  0.002628  0.035864  0.000135  0.001839  0.025089   \n",
      "1       1     1 -0.092742  0.008601 -0.063523 -0.000798  0.005891 -0.043509   \n",
      "2       1     1 -0.213710  0.045672 -0.147941 -0.009761  0.031616 -0.102412   \n",
      "3       1     1 -0.375000  0.140625 -0.188321 -0.052734  0.070620 -0.094573   \n",
      "4       1     1 -0.513250  0.263426 -0.238990 -0.135203  0.122661 -0.111283   \n",
      "\n",
      "       In40      In41      In42      In43  \n",
      "0  0.000007  0.000094  0.001286  0.017551  \n",
      "1  0.000074 -0.000546  0.004035 -0.029801  \n",
      "2  0.002086 -0.006757  0.021886 -0.070895  \n",
      "3  0.019775 -0.026483  0.035465 -0.047494  \n",
      "4  0.069393 -0.062956  0.057116 -0.051818  \n"
     ]
    }
   ],
   "source": [
    "new_features(data, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need new cost and gradient functions which include a penalty parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost_reg(theta, X, y, penalty):\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    theta = np.matrix(theta)\n",
    "    \n",
    "    #print(\"X in cost\",X.shape)\n",
    "    #print(\"theta in cost\", theta.shape)\n",
    "    prob1 = np.multiply(-y, np.log(sigmoid(X * theta.T)))\n",
    "    prob2 = np.multiply((1-y), np.log(1-sigmoid(X * theta.T)))\n",
    "    \n",
    "    reg_term = (penalty / (2 * len(X))) * np.sum(np.power(theta[:,1:theta.shape[1]],2))\n",
    "\n",
    "    return np.sum(prob1-prob2)/(len(X)) + reg_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_reg(theta, X, y, penalty):\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    theta = np.matrix(theta)\n",
    "    \n",
    "    #Get the number of parameters\n",
    "    parameters = int(theta.ravel().shape[1])\n",
    "    grad = np.zeros(parameters)\n",
    "    #print (X.shape)\n",
    "    #print (theta.T.shape)\n",
    "    error = sigmoid(X * theta.T) - y\n",
    "\n",
    "    for i in range(parameters):\n",
    "        term = np.multiply(error, X[:,i])\n",
    "    \n",
    "        if (i == 0):\n",
    "            grad[i] = np.sum(term) / len(X)\n",
    "        else:\n",
    "            grad[i] = (np.sum(term) / len(X)) + ((penalty / len(X)) * theta[:, i])\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Prepare X, y and theta\n",
    "\n",
    "##messed up data above\n",
    "cols = data.shape[1]\n",
    "X = data.iloc[:,1:cols]\n",
    "y = data.iloc[:,0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471805599454"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(X.values)  \n",
    "y = np.array(y.values)  \n",
    "theta = np.zeros(11)\n",
    "\n",
    "lamda = 1\n",
    "\n",
    "cost_reg(theta, X, y, lamda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.53010249,  0.29075567, -1.60725763, -0.5821382 ,  0.01781027,\n",
       "        -0.21329509, -0.40024142, -1.37144139,  0.02264303, -0.9503358 ,\n",
       "         0.0344085 ]), 22, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = opt.fmin_tnc(func=cost_reg, x0=theta, fprime=gradient_reg, args=(X, y, lamda))  \n",
    "output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "theta_min = np.matrix(output[0])  \n",
    "predictions = predict(theta_min, X)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO - \n",
    "1. Add accuracy function\n",
    "2. Plotting\n",
    "3. Decision boundary plotting\n",
    "4. Test data prediction\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
